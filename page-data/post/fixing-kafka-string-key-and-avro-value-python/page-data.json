{"componentChunkName":"component---src-templates-post-js","path":"/post/fixing-kafka-string-key-and-avro-value-python","result":{"data":{"markdownRemark":{"id":"aa064346-1a3a-59b0-b02f-27c997132728","html":"<h1>Fixing Kafka String Key and Avro Value Python</h1>\n<blockquote>\n<blockquote>\n<p>confluent_kafka.avro.serializer.SerializerError: message does not start with magic byte</p>\n</blockquote>\n</blockquote>\n<p>Damn confluent-python... I faced a lot of challenges when trying to set up kafka thing. First, I encountered listener protocol mapping, then KSQL, and now even confluent-kafka. -.-</p>\n<p>The story began when I tried to consume a topic that is processed using KSQL. I tried to raise question at <a href=\"https://stackoverflow.com/questions/57472853/kafka-message-does-not-start-with-magic-byte\">stackoverflow</a> but somehow I have to solve it myself. </p>\n<p>What is the problem. There is a problem occurred when you tried to use KSQL to create stream or table. Your key will be in STRING format whereas your value will be in AVRO format</p>\n<pre><code>ksql> describe extended API_TABLE;\n\nName                 : API_TABLE\nType                 : STREAM\nKey field            : \nKey format           : STRING\nTimestamp field      : CREATED_ON\nValue format         : AVRO\n</code></pre>\n<p>So far, someone proposed a PR on this issue but the solution is not working for me. Why? Because I used god damn WINDOW TUMBLING when processing the data, which means I can't just copy it. This is my data. </p>\n<pre><code>ksql> select * from api_table;\n1565752903000 | api_metric_567 : Window{start=1565752800000 end=-} | 1565752800000 | 1\n1565752923000 | api_metric_23  : Window{start=1565752800000 end=-} | 1565752800000 | 1\n1565752925000 | api_metric_43 : Window{start=1565752800000 end=-}  | 1565752800000 | 1\n1565752935000 | api_metric_54 : Window{start=1565752800000 end=-}  | 1565752800000 | 1\n</code></pre>\n<p>First column is rowtime, second column is my key, third and 4th column are my values. </p>\n<p>When I tried to use its code, the key cannot be decoded into utf-8 because it has binary data causing an error. Window{...} is the culprit. >&#x3C;</p>\n<p>There are two way to solve the problem. One is create a new topic that set value_format as json, second is do some little hack and abandon using the AvroConsumer. And I tried the second one. </p>\n<p>So I spent some time, figuring out what does those bytes means, and read some source code. Let's get started. Write this first. Copy~</p>\n<pre><code>from confluent_kafka import KafkaError\nimport io\nfrom confluent_kafka import Consumer\nfrom confluent_kafka.avro.serializer import SerializerError\n\n\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9021',\n    'group.id': 'abcd'\n})\n\nconsumer.subscribe(['API_TABLE'])\n\nwhile True:\n    try:\n        msg = consumer.poll(10)\n    except SerializerError as e:\n        print(\"Message deserialization failed for {}: {}\".format(msg, e))\n        break\n\n    if msg is None:\n        continue\n\n    if msg.error():\n        print(\"AvroConsumer error: {}\".format(msg.error()))\n        continue\n\n    print(msg.value())\n</code></pre>\n<p>So here's the thing. Please please do not use AvroConsumer. Wait for developers to solve the issue. (Write down date first... 14 August 2019)</p>\n<p>When we try to poll the data, it will get the bytes for key and the bytes for value.</p>\n<pre><code>b'api_metric_24\\x00\\x00\\x01l\\x8fd\"@' b'\\x00\\x00\\x00\\x00\\x04\\x02\\x80\\x89\\xa1\\xf6\\x91[\\x02\\x02'\n</code></pre>\n<p>First row is msg.key(), 2nd row is value.key(). As you can see when you try to use window function, KSQL put serialised timestamp into your key and messed it up. However, we can see that if we split 'api<em>metric</em>24' and '\\x00\\x00\\x01l\\x8fd\"@' out, we can get our key. Now look at the value. It starts to look little ughh...</p>\n<p>According to <a href=\"https://docs.confluent.io/current/schema-registry/serializer-formatter.html#wire-format\">wire format</a>, we can know that for 0 position bytes is magic bytes, 1st to 4th are schemaID, then 5... are data. </p>\n<p>Therefore, we need to pull out magic bytes and schema ID, check and call schema from our schema registry. We will use struct library to interpret these binary code. </p>\n<pre><code>>>> import struct\n>>> msg = b'\\x00\\x00\\x00\\x00\\x04\\x02\\x80\\x89\\xa1\\xf6\\x91[\\x02\\x02'\n>>> struct.unpack('>bI', msg[:5])\n(0, 4)\n</code></pre>\n<p>Yeah!! We just pull out magic bytes and schema ID. Then we use ready-made functions from confluent-kafka. Some functions are still working as expected. ðŸ˜‰</p>\n<pre><code class=\"language-python\">from confluent_kafka.avro.cached_schema_registry_client import CachedSchemaRegistryClient\n\nregister_client = CachedSchemaRegistryClient(url=\"http://localhost:8081\")\n</code></pre>\n<p>We pull the schema using schemaID, then decode and read, then we have nice clean value we want. ðŸ˜€</p>\n<pre><code class=\"language-python\">from avro.io import BinaryDecoder, DatumReader\n\nMAGIC_BYTES = 0\n\ndef unpack(payload):\n    magic, schema_id = struct.unpack('>bi', payload[:5])\n\n    # Get Schema registry\n    # Avro value format\n    if magic == MAGIC_BYTES:\n        schema = register_client.get_by_id(schema_id)\n        reader = DatumReader(schema)\n        output = BinaryDecoder(io.BytesIO(payload[5:]))\n        result = reader.read(output)\n        return result\n        \n    # String key\n    else:\n        # Timestamp is inside my key\n        return payload[:-8].decode()\n</code></pre>\n<p>Noted I just discarded the timestamp, because I had timestamp included in my values. But why -8? This was because KSQL encoded timestamp as <code>long long</code> type. Look at the <a href=\"https://docs.python.org/3.7/library/struct.html#format-characters\">doc</a> for reference.</p>\n<p>If you too lazy to look at my explanation, just copy the code below. Feel free to adjust yourself. </p>\n<script src=\"https://gist.github.com/lee1409/46025cf7d3a3cf8f2c898e40b4c4ed76.js\"></script>","frontmatter":{"date":"May 04, 2019","slug":"/post/fixing-kafka-string-key-and-avro-value-python","title":"Fixing Kafka String Key and Avro Value Python"}}},"pageContext":{"slug":"/post/fixing-kafka-string-key-and-avro-value-python"}}}